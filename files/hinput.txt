#TODO implement a preprocessing, execution, and postprocessing phase
#(1) exec. preconditions: required files
#(2) exec. a command line statement: which might not include all required files
#(3) exec. postconditions: deposit files - these might not be fully stated in the previous execs. 
#
#TODO test empty lines in inputFormat
#
##########################
# input files are automatically retrieved from a URI to the working dir. before the execution. (requires the impl of providers for different schemas like hdfs, file, myRepro, ...)
# output files are automatically written to a URI after the execution. (requires the impl of providers for different schemas)
# note:if it's required to move files from/to subdirectories, this has to be done as part of the workflow
#
#TODO download tar, extract it, exec. cmd, tar and upload results
#TODO e.g. hdfs://paper.ps | ps2pdf - paper.ps.pdf; tar -cf paper.ps.pdf.tar paper.ps.pdf
#TODO resolve a pre/post-condition via stdin/stdout pipes
#TODO e.g. -pre hdfs:///user/rainer/docs/paper.ps?mode=stream
#

--input hdfs:///user/docs/paper.ps --output hdfs:///user/docs/paper.pdf

# deprecated:
#-exec paper.ps paper.ps.pdf -pre hdfs:///user/rainer/docs/paper.ps -post hdfs:///user/rainer/docs/paper.ps.pdf
#NO_HDFS -exec file:///absolute/in.file out.file 
#HDFS_WITH_STREAMS
#-exec /etc/fugol paper.pdf -pre http://files.com/paper.pdf
#A worfklow can be a single (even pipe-connected) command, or a batch job of cmds
#cant't we use scripts as for batch jobs???!!
#->the script would be a pre-condition to download from hdfs
#->but must change chmod to +x
#
#just a dummy comment
#another one...
